import json
import re
from typing import Any, AsyncGenerator, Dict, List, Mapping, Sequence, Union, cast
from typing_extensions import Self
from datetime import datetime
from pydantic import Field
from autogen_agentchat.base import Response
from autogen_agentchat.messages import BaseChatMessage, TextMessage, MultiModalMessage
from autogen_core import CancellationToken
from autogen_core.models import ChatCompletionClient, SystemMessage, UserMessage

from ._web_surfer import WebSurfer, WebSurferConfig, WebSurferState
from ...tools.playwright.browser import PlaywrightBrowser
from loguru import logger as trace_logger


class DeepSearchWebSurferConfig(WebSurferConfig):
    """æ·±åº¦æœç´¢ç½‘é¡µæµè§ˆå™¨é…ç½®"""
    max_pages_per_search: int = 3  # æ¯æ¬¡æœç´¢æœ€å¤§é¡µé¢æ•°
    detailed_analysis: bool = True  # æ˜¯å¦è¿›è¡Œè¯¦ç»†åˆ†æ
    save_search_history: bool = True  # æ˜¯å¦ä¿å­˜æœç´¢å†å²
    research_mode: bool = True  # ç ”ç©¶æ¨¡å¼ï¼Œæ›´æ³¨é‡ä¿¡æ¯æ”¶é›†
    # æ–°å¢æå‰ç»“æŸæœºåˆ¶é…ç½®
    enable_early_termination: bool = True  # æ˜¯å¦å¯ç”¨æå‰ç»“æŸæœºåˆ¶
    min_pages_before_check: int = 2  # æ£€æŸ¥æå‰ç»“æŸå‰æœ€å°‘è®¿é—®çš„é¡µé¢æ•°
    satisfaction_threshold: float = 0.8  # æ»¡è¶³åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
    check_interval: int = 2  # æ¯è®¿é—®å‡ ä¸ªé¡µé¢æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦å¯ä»¥æå‰ç»“æŸ
    max_total_pages: int = 45  # æœ€å¤§æ€»é¡µé¢æ•°é™åˆ¶


class DeepSearchWebSurferState(WebSurferState):
    """æ·±åº¦æœç´¢ç½‘é¡µæµè§ˆå™¨çŠ¶æ€"""
    search_history: List[Dict[str, Any]] = Field(default_factory=list)
    collected_information: List[Dict[str, Any]] = Field(default_factory=list)
    search_depth: int = 0
    visited_urls: List[str] = Field(default_factory=list)  # ä¿å­˜å·²è®¿é—®çš„URLs
    search_queue: List[str] = Field(default_factory=list)  # æœç´¢å…³é”®è¯é˜Ÿåˆ—
    searched_keywords: List[str] = Field(default_factory=list)  # å·²æœç´¢çš„å…³é”®è¯åˆ—è¡¨
    total_pages_visited: int = 0  # æ€»è®¿é—®é¡µé¢æ•°
    page_results: List[str] = Field(default_factory=list)  # é¡µé¢æœç´¢ç»“æœ
    type: str = Field(default="DeepSearchWebSurferState")


class DeepSearchWebSurfer(WebSurfer):
    """æ·±åº¦æœç´¢ç½‘é¡µæµè§ˆå™¨
    
    ä¸“é—¨ç”¨äºè¿›è¡Œæ·±å…¥ç ”ç©¶æœç´¢çš„æ™ºèƒ½ä»£ç†ã€‚ä¸æ™®é€šWebSurferç›¸æ¯”ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š
    1. èƒ½å¤Ÿè¿›è¡Œå¤šå±‚æ¬¡çš„æ·±åº¦æœç´¢
    2. è‡ªåŠ¨æ”¶é›†å’Œæ•´ç†è¯¦ç»†çš„é¡µé¢ä¿¡æ¯
    3. æ”¯æŒå¤šä¸ªæœç´¢å…³é”®è¯çš„ç»¼åˆåˆ†æ
    4. æä¾›æ›´è¯¦ç»†çš„æœç´¢ç»“æœæ‘˜è¦
    5. å…·æœ‰æœç´¢å†å²è®°å½•å’Œä¿¡æ¯æ•´åˆèƒ½åŠ›
    """
    
    component_type = "agent"
    component_config_schema = DeepSearchWebSurferConfig
    component_provider_override = "magentic_ui.agents.web_surfer.DeepSearchWebSurfer"
    
    DEFAULT_DESCRIPTION = """
    æ·±åº¦æœç´¢ç½‘é¡µæµè§ˆå™¨æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¿›è¡Œæ·±å…¥ç ”ç©¶æœç´¢çš„æ™ºèƒ½ä»£ç†ã€‚
    å®ƒèƒ½å¤Ÿï¼š
    - è¿›è¡Œå¤šå±‚æ¬¡çš„æ·±åº¦æœç´¢ï¼ŒæŒ–æ˜æ›´å…¨é¢çš„ä¿¡æ¯
    - è‡ªåŠ¨è®¿é—®å¤šä¸ªç›¸å…³é¡µé¢å¹¶æå–è¯¦ç»†å†…å®¹
    - å¯¹æœç´¢ç»“æœè¿›è¡Œæ™ºèƒ½åˆ†æå’Œæ•´åˆ
    - æ”¯æŒå¤æ‚æŸ¥è¯¢çš„åˆ†è§£å’Œé€æ­¥è§£ç­”
    
    è¯¥ä»£ç†ç‰¹åˆ«é€‚åˆéœ€è¦æ·±å…¥äº†è§£æŸä¸ªä¸»é¢˜ã€è¿›è¡Œå¸‚åœºè°ƒç ”ã€æŠ€æœ¯åˆ†æç­‰åœºæ™¯ã€‚å¯¹æŸä¸€ä¸»é¢˜è¿›è¡Œæ·±å…¥çš„è°ƒæŸ¥ç ”ç©¶æ—¶ï¼Œå»ºè®®ä½¿ç”¨æ·±åº¦æœç´¢ã€‚
    è¯¥ä»£ç†ä¸é€‚åˆç®€å•çš„ä¿¡æ¯æŸ¥è¯¢å’Œç½‘é¡µæ“ä½œï¼Œä¾‹å¦‚ï¼šæŸ¥è¯¢è½¦ç¥¨ã€æœºç¥¨ä¿¡æ¯ã€‚
    """
    
    DEEP_SEARCH_SYSTEM_MESSAGE = """
    æ‚¨æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºæ·±å…¥ç ”ç©¶æœç´¢çš„æ™ºèƒ½åŠ©æ‰‹ã€‚æ‚¨çš„ä»»åŠ¡æ˜¯é’ˆå¯¹ç”¨æˆ·çš„è¯·æ±‚è¿›è¡Œå…¨é¢ã€æ·±å…¥çš„ä¿¡æ¯æ”¶é›†å’Œåˆ†æã€‚

    é‡è¦æç¤ºï¼šé™¤éç”¨æˆ·ç‰¹åˆ«è¦æ±‚å…¶ä»–è¯­è¨€ï¼Œå¦åˆ™è¯·ç”¨ç®€ä½“ä¸­æ–‡å›å¤ã€‚

    æ‚¨çš„æ ¸å¿ƒèƒ½åŠ›ï¼š
    1. æ·±åº¦æœç´¢ï¼šä¸æ»¡è¶³äºè¡¨é¢ä¿¡æ¯ï¼Œä¼šæ·±å…¥æŒ–æ˜ç›¸å…³ä¸»é¢˜
    2. å¤šè§’åº¦åˆ†æï¼šä»ä¸åŒè§’åº¦å’Œå±‚é¢åˆ†æé—®é¢˜
    3. ä¿¡æ¯æ•´åˆï¼šå°†å¤šä¸ªæ¥æºçš„ä¿¡æ¯è¿›è¡Œç»¼åˆåˆ†æ
    4. ç»“æ„åŒ–è¾“å‡ºï¼šæä¾›æ¸…æ™°ã€æœ‰æ¡ç†çš„ç ”ç©¶ç»“æœ

    æœç´¢ç­–ç•¥ï¼š
    - ä½¿ç”¨å¤šä¸ªç›¸å…³å…³é”®è¯è¿›è¡Œæœç´¢
    - è®¿é—®æƒå¨ç½‘ç«™å’Œä¸“ä¸šèµ„æº
    - æ”¶é›†ä¸åŒè§‚ç‚¹å’Œæ•°æ®
    - éªŒè¯ä¿¡æ¯çš„å¯é æ€§å’Œæ—¶æ•ˆæ€§
    - æ•´ç†æˆç»“æ„åŒ–çš„ç ”ç©¶æŠ¥å‘Š

    å½“å‰æ—¥æœŸï¼š{date_today}
    """
    
    def __init__(
        self,
        name: str,
        model_client: ChatCompletionClient,
        browser: PlaywrightBrowser,
        max_pages_per_search: int = 5,
        detailed_analysis: bool = True,
        save_search_history: bool = True,
        research_mode: bool = True,
        # æ–°å¢æå‰ç»“æŸæœºåˆ¶å‚æ•°
        enable_early_termination: bool = True,
        min_pages_before_check: int = 3,
        satisfaction_threshold: float = 0.8,
        check_interval: int = 2,
        # æ–°å¢æœ€å¤§é¡µé¢æ•°é™åˆ¶
        max_total_pages: int = 45,
        **kwargs: Any
    ) -> None:
        """åˆå§‹åŒ–æ·±åº¦æœç´¢ç½‘é¡µæµè§ˆå™¨"""
        super().__init__(name, model_client, browser, **kwargs)
        
        # æ·±åº¦æœç´¢ç‰¹æœ‰é…ç½®
        self.max_pages_per_search = max_pages_per_search
        self.detailed_analysis = detailed_analysis
        self.save_search_history = save_search_history
        self.research_mode = research_mode
        
        # æå‰ç»“æŸæœºåˆ¶é…ç½®
        self.enable_early_termination = enable_early_termination
        self.min_pages_before_check = min_pages_before_check
        self.satisfaction_threshold = satisfaction_threshold
        self.check_interval = check_interval
        # æ–°å¢æœ€å¤§é¡µé¢æ•°é™åˆ¶
        self.max_total_pages = max_total_pages
        
        # æœç´¢çŠ¶æ€
        self.search_history: List[Dict[str, Any]] = []
        self.collected_information: List[Dict[str, Any]] = []
        self.current_search_depth = 0
        # è®°å½•å·²è®¿é—®çš„é“¾æ¥ï¼Œé¿å…é‡å¤è®¿é—®
        self.visited_urls: set[str] = set()
        # æœç´¢å…³é”®è¯é˜Ÿåˆ—
        self.search_queue: List[str] = []
        # å·²æœç´¢çš„å…³é”®è¯åˆ—è¡¨
        self.searched_keywords: List[str] = []
        # æ€»è®¿é—®é¡µé¢æ•°
        self.total_pages_visited: int = 0
        # ä¿å­˜é¡µé¢æœç´¢ç»“æœç”¨äºæœ€ç»ˆè¾“å‡º
        self.page_results: List[str] = []
            
    async def on_messages_stream(
        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken
    ) -> AsyncGenerator[BaseChatMessage | Response, None]:
        """å¤„ç†æ¶ˆæ¯æµï¼Œæ‰§è¡Œæ·±åº¦æœç´¢"""
        
        # é‡ç½®æœç´¢çŠ¶æ€ï¼Œå¼€å§‹æ–°çš„æœç´¢
        # self.visited_urls.clear()
        # self.collected_information.clear()
        # self.search_history.clear()
        self.search_queue.clear() # æ¸…ç©ºé˜Ÿåˆ—
        self.searched_keywords.clear() # æ¸…ç©ºå·²æœç´¢å…³é”®è¯åˆ—è¡¨
        self.total_pages_visited = 0 # é‡ç½®æ€»è®¿é—®é¡µé¢æ•°
        self.page_results.clear() # æ¸…ç©ºé¡µé¢ç»“æœ
        
        # è§£æç”¨æˆ·è¯·æ±‚
        user_query = self._extract_user_query(messages)
        
        if not user_query:
            yield Response(
                chat_message=TextMessage(
                    content="è¯·æä¾›æ‚¨éœ€è¦æ·±å…¥ç ”ç©¶çš„ä¸»é¢˜æˆ–é—®é¢˜ã€‚",
                    source=self.name,
                )
            )
            return
        
        # å¼€å§‹æ·±åº¦æœç´¢æµç¨‹
        yield Response(
            chat_message=TextMessage(
                content=f"ğŸ” å¼€å§‹å¯¹ã€Œ{user_query}ã€è¿›è¡Œæ·±åº¦ç ”ç©¶æœç´¢...",
                source=self.name,
            )
        )
        
        try:
            # æ‰§è¡Œæ·±åº¦æœç´¢
            async for result in self._perform_deep_search(user_query, cancellation_token):
                yield result
            
            # # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            # final_report = await self._generate_research_report(user_query)
            # yield Response(
            #     chat_message=TextMessage(
            #         content=final_report,
            #         source=self.name,
            #         metadata={"type": "research_report", "internal": "no"},
            #     )
            # )
            
        except Exception as e:
            trace_logger.error(f"æ·±åº¦æœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            yield Response(
                chat_message=TextMessage(
                    content=f"æ·±åº¦æœç´¢è¿‡ç¨‹ä¸­é‡åˆ°é”™è¯¯ï¼š{str(e)}",
                    source=self.name,
                )
            )
    
    def _extract_user_query(self, messages: Sequence[BaseChatMessage]) -> str:
        """æå–ç”¨æˆ·æŸ¥è¯¢"""
        if not messages:
            return ""
        
        last_message = messages[-1]
        if isinstance(last_message, TextMessage):
            return last_message.content
        elif isinstance(last_message, MultiModalMessage):
            # æå–æ–‡æœ¬å†…å®¹
            text_content = ""
            for content in last_message.content:
                if isinstance(content, str):
                    text_content += content + " "
            return text_content.strip()
        
        return ""
    
    async def _perform_deep_search(
        self, query: str, cancellation_token: CancellationToken
    ) -> AsyncGenerator[Response, None]:
        """æ‰§è¡Œæ·±åº¦æœç´¢ - åŸºäºé˜Ÿåˆ—çš„åŠ¨æ€æœç´¢ç­–ç•¥"""
        
        # ç”Ÿæˆåˆå§‹æœç´¢å…³é”®è¯å¹¶åŠ å…¥é˜Ÿåˆ—
        initial_keywords = await self._generate_search_keywords(query)
        self.search_queue.extend(initial_keywords)
        
        yield Response(
            chat_message=TextMessage(
                content=f"ğŸ“‹ åˆå§‹æœç´¢é˜Ÿåˆ—ï¼š{', '.join(initial_keywords)}",
                source=self.name,
            )
        )
        
        # å¤„ç†æœç´¢é˜Ÿåˆ—ç›´åˆ°é˜Ÿåˆ—ä¸ºç©ºæˆ–è¾¾åˆ°æœ€å¤§é¡µé¢æ•°
        while self.search_queue and self.total_pages_visited < self.max_total_pages:
            if cancellation_token.is_cancelled():
                break
            
            # ä»é˜Ÿåˆ—ä¸­å–å‡ºä¸‹ä¸€ä¸ªå…³é”®è¯
            current_keyword = self.search_queue.pop(0)
            
            # æ·»åŠ åˆ°å·²æœç´¢åˆ—è¡¨
            if current_keyword not in self.searched_keywords:
                self.searched_keywords.append(current_keyword)
            
            yield Response(
                chat_message=TextMessage(
                    content=f"ğŸ” æ­£åœ¨æœç´¢å…³é”®è¯ï¼š{current_keyword} (å·²è®¿é—®: {self.total_pages_visited} ä¸ªé¡µé¢)",
                    source=self.name,
                )
            )
            
            # æ‰§è¡Œå•ä¸ªå…³é”®è¯æœç´¢
            async for result in self._search_single_keyword(current_keyword, cancellation_token):
                yield result
                
                # è¾¾åˆ°æœ€å¤§é¡µé¢æ•°æ—¶åœæ­¢
                if self.total_pages_visited >= self.max_total_pages:
                    final_summary = "\n\n---\n\n".join(self.page_results)
                    yield Response(
                        chat_message=TextMessage(
                            content=f"æ·±åº¦æœç´¢å®Œæˆï¼šå·²è¾¾åˆ°æœ€å¤§é¡µé¢æ•°é™åˆ¶ ({self.max_total_pages})ã€‚\n{final_summary}",
                            source=self.name,
                        )
                    )
                    return
            
            # åœ¨å•ä¸ªå…³é”®è¯æœç´¢å®Œæˆåè¿›è¡Œæ£€æŸ¥
            if self.total_pages_visited >= self.min_pages_before_check:
                if (self.total_pages_visited - self.min_pages_before_check) % self.check_interval == 0:
                    should_terminate, reason, missing_aspects = await self._check_early_termination_with_missing(query)
                    
                    if should_terminate:
                        # ç›´æ¥æ‹¼æ¥æ‰€æœ‰é¡µé¢ç»“æœ
                        final_summary = "\n\n---\n\n".join(self.page_results)
                        yield Response(
                            chat_message=TextMessage(
                                content=f"æ·±åº¦æœç´¢å®Œæˆï¼š{reason}\n\n{final_summary}",
                                source=self.name,
                                metadata={"type": "final_report", "internal": "no"},
                            )
                        )
                        return
                    else:
                        # æ ¹æ®missing_aspectsç”Ÿæˆæ–°çš„æœç´¢å…³é”®è¯
                        new_keywords = await self._generate_keywords_from_missing_aspects(
                            query, reason, missing_aspects
                        )
                        
                        if new_keywords:
                            # è¿‡æ»¤æ‰å·²ç»æœç´¢è¿‡çš„å…³é”®è¯
                            filtered_keywords = [kw for kw in new_keywords if kw not in self.searched_keywords]
                            if filtered_keywords:
                                # æ›¿æ¢å½“å‰æœç´¢é˜Ÿåˆ—
                                self.search_queue = filtered_keywords
                                yield Response(
                                    chat_message=TextMessage(
                                        content=f"ğŸ“ æ ¹æ®è¯„ä¼°ç»“æœæ›¿æ¢æœç´¢é˜Ÿåˆ—ï¼š{', '.join(filtered_keywords)}",
                                        source=self.name,
                                    )
                                )
                            else:
                                yield Response(
                                    chat_message=TextMessage(
                                        content="ğŸ“ è¯„ä¼°ç”Ÿæˆçš„å…³é”®è¯éƒ½å·²æœç´¢è¿‡ï¼Œç»§ç»­å½“å‰é˜Ÿåˆ—",
                                        source=self.name,
                                    )
                                )
            
            # å¦‚æœæœç´¢é˜Ÿåˆ—ä¸ºç©ºä½†æœªè¾¾åˆ°ç»“æŸæ¡ä»¶ï¼Œæä¾›çŠ¶æ€æ›´æ–°
            if not self.search_queue and self.total_pages_visited < self.max_total_pages:
                yield Response(
                    chat_message=TextMessage(
                        content=f"ğŸ“‹ æœç´¢é˜Ÿåˆ—å·²ç©ºï¼Œå…±è®¿é—® {self.total_pages_visited} ä¸ªé¡µé¢",
                        source=self.name,
                    )
                )
        
        # æœç´¢ç»“æŸ
        if self.total_pages_visited >= self.max_total_pages:
            final_summary = "\n\n---\n\n".join(self.page_results)
            yield Response(
                chat_message=TextMessage(
                    content=f"æ·±åº¦æœç´¢å®Œæˆï¼šå·²è¾¾åˆ°æœ€å¤§é¡µé¢æ•°é™åˆ¶ {final_summary}",
                    source=self.name,
                    metadata={"type": "final_report", "internal": "no"},
                )
            )
        else:
            final_summary = "\n\n---\n\n".join(self.page_results)
            yield Response(
                chat_message=TextMessage(
                    content=f"æ·±åº¦æœç´¢å®Œæˆï¼šé˜Ÿåˆ—å·²ç©ºï¼Œ{final_summary}",
                    source=self.name,
                    metadata={"type": "final_report", "internal": "no"},
                )
            )
    
    async def _generate_search_keywords(self, query: str) -> List[str]:
        """ç”Ÿæˆæœç´¢å…³é”®è¯"""
        try:
            prompt = f"""
            åŸºäºä»¥ä¸‹æŸ¥è¯¢ï¼Œç”Ÿæˆ3ä¸ªæœ€æ ¸å¿ƒçš„æœç´¢å…³é”®è¯ï¼Œç”¨äºæ·±åº¦ç ”ç©¶ï¼š
            
            æŸ¥è¯¢ï¼š{query}
            
            è¯·ç”Ÿæˆæœ€ç›¸å…³ã€æœ€æœ‰ä»·å€¼çš„3ä¸ªå…³é”®è¯ï¼ŒåŒ…æ‹¬ï¼š
            1. æœ€ç›´æ¥ç›¸å…³çš„æ ¸å¿ƒå…³é”®è¯
            2. ç›¸å…³çš„é‡è¦æ¦‚å¿µæˆ–æœ¯è¯­
            3. èƒ½å¤Ÿè¡¥å……ç¬¬ä¸€ä¸ªå…³é”®è¯çš„æ‰©å±•è¯æ±‡
            
            æ³¨æ„ï¼š
            - åªéœ€è¦3ä¸ªå…³é”®è¯ï¼Œä¸è¦æ›´å¤š
            - å…³é”®è¯åº”è¯¥å…·æœ‰äº’è¡¥æ€§ï¼Œè¦†ç›–ä¸åŒè§’åº¦
            - é¿å…è¿‡äºç›¸ä¼¼çš„å…³é”®è¯
            
            è¯·ä»¥JSONæ ¼å¼è¿”å›å…³é”®è¯åˆ—è¡¨ï¼š
            {{"keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3"]}}
            """
            
            messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æœç´¢ç­–ç•¥ä¸“å®¶ã€‚"),
                UserMessage(content=prompt, source=self.name)
            ]
            for message in messages:
                trace_logger.info(f"ç”Ÿæˆæœç´¢å…³é”®è¯æ¶ˆæ¯: {message.content}")
            response = await self._model_client.create(messages)
            trace_logger.info(f"ç”Ÿæˆæœç´¢å…³é”®è¯å“åº”: {response.content}")
            # è§£æå“åº”
            if isinstance(response.content, str):
                try:
                    # è§£æå“åº”ï¼Œå»é™¤```json ```
                    content = response.content.replace("```json", "").replace("```", "").strip()
                    result = json.loads(content)
                    keywords = result.get("keywords", [query])
                    # ç¡®ä¿ä¸è¶…è¿‡3ä¸ªå…³é”®è¯
                    return keywords[:3]
                except json.JSONDecodeError:
                    return [query]
            
            return [query]
            
        except Exception as e:
            trace_logger.error(f"ç”Ÿæˆæœç´¢å…³é”®è¯å¤±è´¥: {e}")
            return [query]
    
    async def _check_early_termination_with_missing(self, original_query: str) -> tuple[bool, str, List[str]]:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥æå‰ç»“æŸæœç´¢ï¼Œå¹¶è¿”å›ç¼ºå¤±æ–¹é¢
        
        Args:
            original_query: åŸå§‹æŸ¥è¯¢
            
        Returns:
            tuple[bool, str, List[str]]: (æ˜¯å¦åº”è¯¥ç»“æŸ, ç»“æŸåŸå› , ç¼ºå¤±çš„æ–¹é¢åˆ—è¡¨)
        """
        try:
            if not self.collected_information:
                return False, "æ— æ”¶é›†ä¿¡æ¯", []
            
            # å‡†å¤‡å·²æ”¶é›†ä¿¡æ¯çš„æ‘˜è¦
            info_summaries: List[str] = []
            for info in self.collected_information:
                summary = info.get('summary', '')
                key_points = info.get('key_points', [])
                title = info.get('title', 'æœªçŸ¥')
                
                # å®‰å…¨åœ°å¤„ç†key_pointsåˆ—è¡¨ç±»å‹
                if not key_points:
                    key_points_list: List[str] = []
                elif isinstance(key_points, list):
                    # æ˜¾å¼è½¬æ¢æ¯ä¸ªå…ƒç´ ä¸ºå­—ç¬¦ä¸²
                    key_points_list = []
                    for item in key_points:  # type: ignore
                        if item is not None:
                            key_points_list.append(str(item))  # type: ignore
                else:
                    key_points_list = [str(key_points)]
                
                key_points_str = ', '.join(key_points_list)
                
                info_summaries.append(f"æ ‡é¢˜: {title}\næ‘˜è¦: {summary}\nå…³é”®ç‚¹: {key_points_str}")
            
            collected_info_text = "\n\n".join(info_summaries)
            
            # ä½¿ç”¨LLMè¯„ä¼°ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿ
            evaluation_prompt = f"""
            ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶åˆ†æå¸ˆï¼Œè¯·è¯„ä¼°å·²æ”¶é›†çš„ä¿¡æ¯æ˜¯å¦è¶³ä»¥å›ç­”ç”¨æˆ·çš„åŸå§‹æŸ¥è¯¢ã€‚
            
            åŸå§‹æŸ¥è¯¢ï¼š{original_query}
            
            å·²æ”¶é›†ä¿¡æ¯ï¼ˆå…±{len(self.collected_information)}ä¸ªé¡µé¢ï¼‰ï¼š
            {collected_info_text}
            
            è¯·ä»ä»¥ä¸‹å‡ ä¸ªç»´åº¦è¯„ä¼°ä¿¡æ¯çš„å……åˆ†æ€§ï¼š
            1. ä¿¡æ¯è¦†ç›–åº¦ï¼šæ˜¯å¦æ¶µç›–äº†æŸ¥è¯¢çš„ä¸»è¦æ–¹é¢
            2. ä¿¡æ¯æ·±åº¦ï¼šæ˜¯å¦æä¾›äº†è¶³å¤Ÿè¯¦ç»†çš„ä¿¡æ¯
            3. ä¿¡æ¯è´¨é‡ï¼šä¿¡æ¯æ˜¯å¦å¯é å’Œæƒå¨
            4. ä¿¡æ¯å®Œæ•´æ€§ï¼šæ˜¯å¦æœ‰æ˜æ˜¾çš„ä¿¡æ¯ç¼ºå£
            5. å¤šæ ·æ€§ï¼šæ˜¯å¦åŒ…å«äº†ä¸åŒè§’åº¦çš„è§‚ç‚¹
            
            è¯·ä»¥JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼š
            {{
                "sufficient": true/false,
                "confidence": 0.0-1.0,
                "coverage_score": 0.0-1.0,
                "depth_score": 0.0-1.0,
                "quality_score": 0.0-1.0,
                "completeness_score": 0.0-1.0,
                "diversity_score": 0.0-1.0,
                "overall_score": 0.0-1.0,
                "reason": "è¯¦ç»†çš„è¯„ä¼°ç†ç”±",
                "missing_aspects": ["ç¼ºå¤±çš„æ–¹é¢1", "ç¼ºå¤±çš„æ–¹é¢2"]
            }}
            
            æ³¨æ„ï¼š
            - å¦‚æœoverall_score >= {self.satisfaction_threshold}ï¼Œåˆ™sufficientåº”ä¸ºtrue
            - è¯·ä¸¥æ ¼è¯„ä¼°ï¼Œä¸è¦è¿‡äºå®½æ¾
            - è€ƒè™‘ç”¨æˆ·æŸ¥è¯¢çš„å¤æ‚æ€§å’Œæ·±åº¦è¦æ±‚
            - missing_aspectsåº”è¯¥å…·ä½“æ˜ç¡®ï¼Œä¾¿äºç”Ÿæˆæ–°çš„æœç´¢å…³é”®è¯
            """
            
            messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶åˆ†æå¸ˆï¼Œæ“…é•¿è¯„ä¼°ä¿¡æ¯çš„å……åˆ†æ€§å’Œè´¨é‡ã€‚"),
                UserMessage(content=evaluation_prompt, source=self.name)
            ]
            
            trace_logger.info(f"æ£€æŸ¥æå‰ç»“æŸæ¡ä»¶æ¶ˆæ¯: {evaluation_prompt}")
            response = await self._model_client.create(messages)
            trace_logger.info(f"æ£€æŸ¥æå‰ç»“æŸæ¡ä»¶å“åº”: {response.content}")
            
            if isinstance(response.content, str):
                try:
                    # è§£æå“åº”ï¼Œå»é™¤```json ```
                    content = response.content.replace("```json", "").replace("```", "").strip()
                    evaluation = json.loads(content)
                    
                    sufficient = evaluation.get("sufficient", False)
                    overall_score = evaluation.get("overall_score", 0.0)
                    reason = evaluation.get("reason", "è¯„ä¼°å®Œæˆ")
                    missing_aspects = evaluation.get("missing_aspects", [])
                    
                    # è®°å½•è¯„ä¼°ç»“æœ
                    trace_logger.info(f"ä¿¡æ¯å……åˆ†æ€§è¯„ä¼° - åˆ†æ•°: {overall_score}, æ˜¯å¦å……åˆ†: {sufficient}")
                    
                    if sufficient and overall_score >= self.satisfaction_threshold:
                        return True, f"ä¿¡æ¯å·²è¶³å¤Ÿå……åˆ†ï¼ˆè¯„åˆ†: {overall_score:.2f}ï¼‰ã€‚{reason}", missing_aspects
                    else:
                        return False, f"ä¿¡æ¯å°šä¸å……åˆ†ï¼ˆè¯„åˆ†: {overall_score:.2f}ï¼‰{reason}", missing_aspects
                        
                except json.JSONDecodeError:
                    trace_logger.error("è§£æè¯„ä¼°ç»“æœJSONå¤±è´¥")
                    return False, "è¯„ä¼°ç»“æœè§£æå¤±è´¥", []
            
            return False, "è¯„ä¼°å“åº”æ— æ•ˆ", []
            
        except Exception as e:
            trace_logger.error(f"æ£€æŸ¥æå‰ç»“æŸæ¡ä»¶å¤±è´¥: {e}")
            return False, f"è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {str(e)}", []
    
    async def _generate_keywords_from_missing_aspects(
        self, original_query: str, evaluation_reason: str, missing_aspects: List[str]
    ) -> List[str]:
        """æ ¹æ®ç¼ºå¤±çš„æ–¹é¢ç”Ÿæˆæ–°çš„æœç´¢å…³é”®è¯
        
        Args:
            original_query: åŸå§‹æŸ¥è¯¢
            evaluation_reason: è¯„ä¼°ç†ç”±
            missing_aspects: ç¼ºå¤±çš„æ–¹é¢åˆ—è¡¨
            
        Returns:
            List[str]: æ–°çš„æœç´¢å…³é”®è¯åˆ—è¡¨
        """
        try:
            if not missing_aspects:
                return []
            
            missing_aspects_text = ', '.join(missing_aspects)
            
            prompt = f"""
            åŸºäºæœç´¢è¯„ä¼°ç»“æœï¼Œç”Ÿæˆæ–°çš„æœç´¢å…³é”®è¯æ¥è¡¥å……ç¼ºå¤±çš„ä¿¡æ¯ï¼š
            
            åŸå§‹æŸ¥è¯¢ï¼š{original_query}
            è¯„ä¼°ç†ç”±ï¼š{evaluation_reason}
            ç¼ºå¤±çš„æ–¹é¢ï¼š{missing_aspects_text}
            å†å²æœç´¢å…³é”®è¯ï¼š{', '.join(self.searched_keywords)}
            
            è¯·é’ˆå¯¹è¿™äº›ç¼ºå¤±çš„æ–¹é¢ï¼Œç”Ÿæˆ2-3ä¸ªæ–°çš„æœç´¢å…³é”®è¯ã€‚è¦æ±‚ï¼š
            1. ç›´æ¥é’ˆå¯¹ç¼ºå¤±çš„æ–¹é¢
            2. å…·ä½“è€Œæœ‰é’ˆå¯¹æ€§
            3. èƒ½å¤Ÿæ‰¾åˆ°äº’è¡¥çš„ä¿¡æ¯
            4. é¿å…ä¸ä¹‹å‰çš„æœç´¢å…³é”®è¯é‡å¤
            
            è¯·ä»¥JSONæ ¼å¼è¿”å›å…³é”®è¯åˆ—è¡¨ï¼š
            {{"keywords": ["é’ˆå¯¹æ€§å…³é”®è¯1", "é’ˆå¯¹æ€§å…³é”®è¯2", "é’ˆå¯¹æ€§å…³é”®è¯3"]}}
            """
            
            messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æœç´¢ç­–ç•¥ä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®ä¿¡æ¯ç¼ºå£ç”Ÿæˆé’ˆå¯¹æ€§çš„æœç´¢å…³é”®è¯ã€‚"),
                UserMessage(content=prompt, source=self.name)
            ]
            
            trace_logger.info(f"ç”Ÿæˆæ–°å…³é”®è¯è¾“å…¥: {prompt}")
            response = await self._model_client.create(messages)
            trace_logger.info(f"ç”Ÿæˆæ–°å…³é”®è¯å“åº”: {response.content}")
            
            if isinstance(response.content, str):
                try:
                    # è§£æå“åº”ï¼Œå»é™¤```json ```
                    content = response.content.replace("```json", "").replace("```", "").strip()
                    result = json.loads(content)
                    new_keywords = result.get("keywords", [])
                    # é™åˆ¶å…³é”®è¯æ•°é‡ï¼Œæœ€å¤š3ä¸ª
                    return new_keywords[:3]
                except json.JSONDecodeError:
                    trace_logger.error("è§£ææ–°å…³é”®è¯JSONå¤±è´¥")
                    return []
            
            return []
            
        except Exception as e:
            trace_logger.error(f"æ ¹æ®ç¼ºå¤±æ–¹é¢ç”Ÿæˆå…³é”®è¯å¤±è´¥: {e}")
            return []
    
    async def _search_single_keyword(
        self, keyword: str, cancellation_token: CancellationToken
    ) -> AsyncGenerator[Response, None]:
        """æœç´¢å•ä¸ªå…³é”®è¯"""
        
        try:
            # ç›´æ¥è°ƒç”¨webæœç´¢æ–¹æ³•æ›´æ–°é¡µé¢çŠ¶æ€
            search_result = await self._perform_web_search(keyword)
            
            yield Response(
                chat_message=TextMessage(
                    content=f"ğŸ” å·²æœç´¢å…³é”®è¯ã€Œ{keyword}ã€: {search_result}",
                    source=self.name,
                )
            )
            
            async for result in self._extract_and_visit_links_with_output(cancellation_token):
                yield result
                        
        except Exception as e:
            trace_logger.error(f"æœç´¢å…³é”®è¯ '{keyword}' æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            yield Response(
                chat_message=TextMessage(
                    content=f"æœç´¢å…³é”®è¯ '{keyword}' æ—¶é‡åˆ°é”™è¯¯ï¼š{str(e)}",
                    source=self.name,
                )
            )
    
    async def _perform_web_search(self, query: str) -> str:
        """æ‰§è¡Œwebæœç´¢å¹¶æ›´æ–°é¡µé¢çŠ¶æ€"""
        # ç¡®ä¿æµè§ˆå™¨å·²åˆå§‹åŒ–
        if not self.did_lazy_init:
            await self.lazy_init()
        
        # ç›´æ¥è°ƒç”¨webæœç´¢å·¥å…·
        search_args = {"query": query}
        result = await self._execute_tool_web_search(search_args)
        
        return result
    
    async def _extract_and_visit_links_with_output(self, cancellation_token: CancellationToken) -> AsyncGenerator[Response, None]:
        """æå–å¹¶è®¿é—®ç›¸å…³é“¾æ¥ï¼ŒåŒæ—¶è¾“å‡ºæ ¼å¼åŒ–ç»“æœ"""
        if not self._page:
            return
        
        try:
            trace_logger.info(f"é¡µé¢ä¿¡æ¯ï¼š{self._page.url}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯Bingæœç´¢ç»“æœé¡µé¢
            current_url = self._page.url
            if "bing.com/search" in current_url:
                # ä¸“é—¨é’ˆå¯¹Bingæœç´¢ç»“æœé¡µé¢æå–é“¾æ¥
                links = await self._page.evaluate(r"""
                    () => {
                        // æŸ¥æ‰¾æœç´¢ç»“æœé“¾æ¥ - Bingçš„æœç´¢ç»“æœé€šå¸¸åœ¨ç‰¹å®šçš„é€‰æ‹©å™¨ä¸­
                        const resultSelectors = [
                            'h2 a[href]',  // æ ‡é¢˜é“¾æ¥
                            '.b_algo h2 a[href]',  // æ ‡å‡†æœç´¢ç»“æœ
                            '.b_title a[href]',  // æ ‡é¢˜é“¾æ¥
                            '[data-onclick] a[href]',  // å¸¦ç‚¹å‡»äº‹ä»¶çš„é“¾æ¥
                            '.b_entityTP a[href]',  // å®ä½“å¡ç‰‡é“¾æ¥
                            '.b_rich a[href]'  // å¯Œåª’ä½“ç»“æœé“¾æ¥
                        ];
                        
                        const links = [];
                        
                        for (const selector of resultSelectors) {
                            const elements = Array.from(document.querySelectorAll(selector));
                            for (const element of elements) {
                                const href = element.href;
                                const text = element.textContent.trim();
                                const title = element.title || text;
                                
                                // è¿‡æ»¤æ‰æ— æ•ˆé“¾æ¥
                                if (href && 
                                    href.startsWith('http') && 
                                    !href.includes('bing.com') &&  // æ’é™¤Bingè‡ªèº«é“¾æ¥
                                    !href.includes('microsoft.com') && // æ’é™¤å¾®è½¯é“¾æ¥
                                    text.length > 0 && 
                                    text.length < 200 &&
                                    !text.toLowerCase().includes('skip') &&
                                    !text.toLowerCase().includes('privacy') &&
                                    !text.toLowerCase().includes('terms')) {
                                    
                                    links.push({
                                        href: href,
                                        text: text,
                                        title: title
                                    });
                                }
                            }
                        }
                        
                        // å»é‡å¹¶é™åˆ¶æ•°é‡
                        const uniqueLinks = [];
                        const seenUrls = new Set();
                        
                        for (const link of links) {
                            // æ¸…ç†URLï¼Œå»é™¤ç‰‡æ®µæ ‡è¯†ç¬¦å’ŒæŸ¥è¯¢å‚æ•°
                            const cleanUrl = link.href.split('#')[0].split('?')[0].replace(/\/$/, '');
                            
                            if (!seenUrls.has(cleanUrl) && uniqueLinks.length < 10) {
                                seenUrls.add(cleanUrl);
                                uniqueLinks.push({
                                    href: link.href,
                                    text: link.text,
                                    title: link.title,
                                    cleanUrl: cleanUrl  // æ·»åŠ æ¸…ç†åçš„URLç”¨äºåç»­åˆ¤æ–­
                                });
                            }
                        }
                        
                        return uniqueLinks;
                    }
                """)
            else:
                # å¯¹äºéæœç´¢é¡µé¢ï¼Œä½¿ç”¨é€šç”¨çš„é“¾æ¥æå–
                links = await self._page.evaluate("""
                    () => {
                        const links = Array.from(document.querySelectorAll('a[href]'));
                        return links.slice(0, 10).map(link => ({
                            href: link.href,
                            text: link.textContent.trim(),
                            title: link.title || link.textContent.trim()
                        })).filter(link => 
                            link.href.startsWith('http') && 
                            link.text.length > 0 && 
                            link.text.length < 100
                        );
                    }
                """)
            
            trace_logger.info(f"æå–åˆ° {len(links)} ä¸ªæœç´¢ç»“æœé“¾æ¥: {[link['href'] for link in links]}")
            
            if not links:
                yield Response(
                    chat_message=TextMessage(
                        content="âš ï¸ æœªæ‰¾åˆ°å¯è®¿é—®çš„æœç´¢ç»“æœé“¾æ¥",
                        source=self.name,
                    )
                )
                return
            
            # åœ¨éå†å‰è¿‡æ»¤æ‰å·²ç»è®¿é—®è¿‡çš„URL
            unvisited_links: List[Dict[str, str]] = []
            for link in links:
                # æ¸…ç†URLï¼Œå»é™¤ç‰‡æ®µæ ‡è¯†ç¬¦å’ŒæŸ¥è¯¢å‚æ•°ä¸­çš„æ— å…³éƒ¨åˆ†
                clean_url = link['href'].split('#')[0].split('?')[0]
                if clean_url.endswith('/'):
                    clean_url = clean_url[:-1]
                
                # æ£€æŸ¥æ˜¯å¦å·²ç»è®¿é—®è¿‡è¿™ä¸ªURL
                if clean_url not in self.visited_urls:
                    unvisited_links.append({
                        'href': link['href'],
                        'text': link.get('text', ''),
                        'title': link.get('title', ''),
                        'clean_url': clean_url
                    })
                    # æ ‡è®°ä¸ºå·²è®¿é—®
                    self.visited_urls.add(clean_url)
                else:
                    trace_logger.info(f"è·³è¿‡å·²è®¿é—®çš„URL: {clean_url}")
            
            if not unvisited_links:
                yield Response(
                    chat_message=TextMessage(
                        content="âš ï¸ æ‰€æœ‰æœç´¢ç»“æœé“¾æ¥éƒ½å·²è®¿é—®è¿‡ï¼Œè·³è¿‡",
                        source=self.name,
                    )
                )
                return
            
            trace_logger.info(f"è¿‡æ»¤åå‰©ä½™ {len(unvisited_links)} ä¸ªæœªè®¿é—®é“¾æ¥")
            
            # è®¿é—®å‰å‡ ä¸ªæœªè®¿é—®çš„é“¾æ¥
            for i, link in enumerate(unvisited_links[:self.max_pages_per_search]):  # é™åˆ¶è®¿é—®å‰3ä¸ªé“¾æ¥
                if cancellation_token.is_cancelled():
                    break
                try:
                    yield Response(
                        chat_message=TextMessage(
                            content=f"ğŸ“„ æ­£åœ¨è®¿é—®æœç´¢ç»“æœ {i+1}: {link['title'][:50]}...",
                            source=self.name,
                        )
                    )
                    async for result in self._visit_and_analyze_page_with_output(link['href'], link['title'], cancellation_token):
                        yield result
                except Exception as e:
                    trace_logger.error(f"è®¿é—®é“¾æ¥å¤±è´¥: {e}")
                    yield Response(
                        chat_message=TextMessage(
                            content=f"âŒ è®¿é—®é“¾æ¥å¤±è´¥: {link['href'][:50]}...",
                            source=self.name,
                        )
                    )
                    continue
                    
        except Exception as e:
            trace_logger.error(f"æå–é“¾æ¥å¤±è´¥: {e}")
            yield Response(
                chat_message=TextMessage(
                    content=f"âŒ æå–æœç´¢ç»“æœé“¾æ¥æ—¶å‡ºé”™: {str(e)}",
                    source=self.name,
                )
            )
    
    async def _visit_and_analyze_page_with_output(self, url: str, title: str, cancellation_token: CancellationToken) -> AsyncGenerator[Response, None]:
        """è®¿é—®å¹¶åˆ†æé¡µé¢ï¼ŒåŒæ—¶è¾“å‡ºæ ¼å¼åŒ–ç»“æœ"""
        if not self._page:
            return
        
        try:
            # æ£€æŸ¥URLæ˜¯å¦è¢«å…è®¸
            _, approved = await self._check_url_and_generate_msg(url)
            if not approved:
                return
            
            # è®¿é—®é¡µé¢
            await self._playwright_controller.visit_page(self._page, url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            await self._page.wait_for_load_state("domcontentloaded")
            
            # å¢åŠ é¡µé¢è®¿é—®è®¡æ•°
            self.total_pages_visited += 1
            
            # æå–é¡µé¢ä¿¡æ¯
            page_info = await self._extract_detailed_page_info()
            # urlå¤„ç†ï¼Œå»é™¤æ‰#:~:text=é«˜äº®ç¬¦å·åçš„text
            page_info['url'] = url.split('#:~:text=')[0]
            page_info['title'] = title
            
            trace_logger.info(f"æå–é¡µé¢ä¿¡æ¯: {page_info}")
            
            # æ·»åŠ åˆ°æ”¶é›†çš„ä¿¡æ¯ä¸­
            self.collected_information.append(page_info)
            
            
            # TODO æ­¤å¤„è¾“å‡ºçš„ç»“æœè¦ä¿å­˜ï¼Œè¾“å‡ºæ ¼å¼åŒ–çš„æœç´¢ç»“æœ
            formatted_output = await self._format_search_result(page_info)
            self.page_results.append(formatted_output)
            yield Response(
                chat_message=TextMessage(
                    content=formatted_output,
                    source=self.name,
                    metadata={"type": "search_result", "url": url, "internal": "no"},
                )
            )
            
            # è®°å½•æœç´¢å†å²
            if self.save_search_history:
                self.search_history.append({
                    'action': 'visit_page',
                    'url': url,
                    'title': title,
                    'timestamp': datetime.now().isoformat(),
                    'info_collected': len(page_info.get('key_points', []))
                })
            
        except Exception as e:
            trace_logger.error(f"è®¿é—®å’Œåˆ†æé¡µé¢å¤±è´¥: {e}")
            yield Response(
                chat_message=TextMessage(
                    content=f"è®¿é—®é¡µé¢ {url} æ—¶é‡åˆ°é”™è¯¯ï¼š{str(e)}",
                    source=self.name,
                )
            )
    
    async def _format_search_result(self, page_info: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æœç´¢ç»“æœ"""
        try:
            url = page_info.get('url', 'æœªçŸ¥')
            markdown_summary = page_info.get('markdown_summary', 'æ— å†…å®¹æ€»ç»“')
            title = page_info.get('title', 'æœªçŸ¥')
            
            # æ„å»ºè¾“å‡ºæ ¼å¼
            formatted_output = f"""URL: {url}\næ ‡é¢˜: {title}\nå†…å®¹æ€»ç»“:\n{markdown_summary}"""
            
            return formatted_output
            
        except Exception as e:
            trace_logger.error(f"æ ¼å¼åŒ–æœç´¢ç»“æœå¤±è´¥: {e}")
            return f"æ ¼å¼åŒ–ç»“æœæ—¶å‡ºé”™ï¼š{str(e)}"
    
    async def _extract_detailed_page_info(self) -> Dict[str, Any]:
        """æå–è¯¦ç»†çš„é¡µé¢ä¿¡æ¯"""
        if not self._page:
            return {}
        
        try:
            # è·å–é¡µé¢å†…å®¹
            page_content = await self._playwright_controller.get_page_markdown(self._page)
            page_title = await self._page.title()
            
            # å»é™¤è¶…é“¾æ¥ï¼Œä¿ç•™é“¾æ¥æ–‡æœ¬
            # åŒ¹é… [æ–‡æœ¬](é“¾æ¥) æ ¼å¼çš„è¶…é“¾æ¥
            page_content = re.sub(r'\[([^\]]*)\]\([^)]*\)', r'\1', page_content)
            # åŒ¹é… <é“¾æ¥> æ ¼å¼çš„è£¸é“¾æ¥
            page_content = re.sub(r'<[^>]*>', '', page_content)
            # åŒ¹é… http/https å¼€å¤´çš„è£¸é“¾æ¥
            page_content = re.sub(r'https?://[^\s\]]+', '', page_content)
            
            # é™åˆ¶å†…å®¹é•¿åº¦
            if len(page_content) > 3000:
                page_content = page_content[:3000] + "..."
            
            # ä½¿ç”¨LLMåˆ†æé¡µé¢å†…å®¹å¹¶ç”ŸæˆMarkdownæ ¼å¼çš„æ€»ç»“
            analysis_prompt = f"""
            è¯·åˆ†æä»¥ä¸‹ç½‘é¡µå†…å®¹ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªç»“æ„åŒ–çš„Markdownæ ¼å¼å†…å®¹æ€»ç»“ï¼š
            
            é¡µé¢æ ‡é¢˜ï¼š{page_title}
            é¡µé¢å†…å®¹ï¼š
            {page_content}
            
            è¯·æä¾›ä»¥ä¸‹ä¿¡æ¯çš„JSONæ ¼å¼ï¼š
            {{
                "summary": "é¡µé¢å†…å®¹ç®€è¦æ‘˜è¦ï¼ˆ100å­—ä»¥å†…ï¼‰",
                "markdown_summary": "è¯¦ç»†çš„Markdownæ ¼å¼å†…å®¹æ€»ç»“ï¼ŒåŒ…å«ä¸»è¦ç« èŠ‚ã€å…³é”®ç‚¹å’Œé‡è¦ä¿¡æ¯",
                "key_points": ["å…³é”®ç‚¹1", "å…³é”®ç‚¹2", "å…³é”®ç‚¹3"],
                "important_data": ["é‡è¦æ•°æ®1", "é‡è¦æ•°æ®2"],
                "relevant_topics": ["ç›¸å…³ä¸»é¢˜1", "ç›¸å…³ä¸»é¢˜2"],
                "credibility": "ä¿¡æ¯å¯ä¿¡åº¦è¯„ä¼°"
            }}
            
            markdown_summaryå­—æ®µè¦æ±‚ï¼š
            1. ä½¿ç”¨æ ‡å‡†Markdownæ ¼å¼
            2. åŒ…å«é€‚å½“çš„æ ‡é¢˜å±‚çº§ï¼ˆ##ã€###ç­‰ï¼‰
            3. ä½¿ç”¨åˆ—è¡¨ã€ç²—ä½“ã€æ–œä½“ç­‰æ ¼å¼åŒ–å…ƒç´ 
            4. ç»“æ„æ¸…æ™°ï¼Œæ˜“äºé˜…è¯»
            """
            
            messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹åˆ†æå¸ˆï¼Œæ“…é•¿ç”Ÿæˆç»“æ„åŒ–çš„Markdownæ ¼å¼æ€»ç»“ã€‚"),
                UserMessage(content=analysis_prompt, source=self.name)
            ]
            trace_logger.info(f"åˆ†æé¡µé¢å†…å®¹è¾“å…¥: {messages}")
            response = await self._model_client.create(messages)
            trace_logger.info(f"åˆ†æé¡µé¢å†…å®¹ç»“æœ: {response}")
            if isinstance(response.content, str):
                try:
                    # è§£æå“åº”ï¼Œå»é™¤```json ```
                    content = response.content.replace("```json", "").replace("```", "").strip()
                    analysis_result = json.loads(content)
                    return analysis_result
                except json.JSONDecodeError:
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
                    return {
                        "summary": page_content[:200] + "...",
                        "markdown_summary": f"## {page_title}\n\n{page_content}...",
                        "key_points": [],
                        "important_data": [],
                        "relevant_topics": [],
                        "credibility": "æœªè¯„ä¼°"
                    }
            
            return {}
            
        except Exception as e:
            trace_logger.error(f"æå–é¡µé¢ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
     
            
    
    def _to_config(self) -> DeepSearchWebSurferConfig:
        """è½¬æ¢ä¸ºé…ç½®å¯¹è±¡"""
        base_config = super()._to_config()
        return DeepSearchWebSurferConfig(
            **base_config.model_dump(),
            max_pages_per_search=self.max_pages_per_search,
            detailed_analysis=self.detailed_analysis,
            save_search_history=self.save_search_history,
            research_mode=self.research_mode,
            enable_early_termination=self.enable_early_termination,
            min_pages_before_check=self.min_pages_before_check,
            satisfaction_threshold=self.satisfaction_threshold,
            check_interval=self.check_interval,
            max_total_pages=self.max_total_pages,
        )
    
    @classmethod
    def _from_config(cls, config: Union[WebSurferConfig, DeepSearchWebSurferConfig]) -> Self:
        """ä»é…ç½®åˆ›å»ºå®ä¾‹"""
        if isinstance(config, DeepSearchWebSurferConfig):
            return cls(
                name=config.name,
                model_client=ChatCompletionClient.load_component(config.model_client),
                browser=PlaywrightBrowser.load_component(config.browser),
                model_context_token_limit=config.model_context_token_limit,
                downloads_folder=config.downloads_folder,
                description=config.description or cls.DEFAULT_DESCRIPTION,
                debug_dir=config.debug_dir,
                start_page=config.start_page or cls.DEFAULT_START_PAGE,
                animate_actions=config.animate_actions,
                to_save_screenshots=config.to_save_screenshots,
                max_actions_per_step=config.max_actions_per_step,
                to_resize_viewport=config.to_resize_viewport,
                url_statuses=config.url_statuses,
                url_block_list=config.url_block_list,
                single_tab_mode=config.single_tab_mode,
                json_model_output=config.json_model_output,
                multiple_tools_per_call=config.multiple_tools_per_call,
                viewport_height=config.viewport_height,
                viewport_width=config.viewport_width,
                use_action_guard=config.use_action_guard,
                max_pages_per_search=config.max_pages_per_search,
                detailed_analysis=config.detailed_analysis,
                save_search_history=config.save_search_history,
                research_mode=config.research_mode,
                enable_early_termination=config.enable_early_termination,
                min_pages_before_check=config.min_pages_before_check,
                satisfaction_threshold=config.satisfaction_threshold,
                check_interval=config.check_interval,
                max_total_pages=config.max_total_pages,
            )
        else:
            # å¦‚æœæ˜¯åŸºç¡€é…ç½®ï¼Œä½¿ç”¨é»˜è®¤çš„æ·±åº¦æœç´¢å‚æ•°
            return cls(
                name=config.name,
                model_client=ChatCompletionClient.load_component(config.model_client),
                browser=PlaywrightBrowser.load_component(config.browser),
                model_context_token_limit=config.model_context_token_limit,
                downloads_folder=config.downloads_folder,
                description=config.description or cls.DEFAULT_DESCRIPTION,
                debug_dir=config.debug_dir,
                start_page=config.start_page or cls.DEFAULT_START_PAGE,
                animate_actions=config.animate_actions,
                to_save_screenshots=config.to_save_screenshots,
                max_actions_per_step=config.max_actions_per_step,
                to_resize_viewport=config.to_resize_viewport,
                url_statuses=config.url_statuses,
                url_block_list=config.url_block_list,
                single_tab_mode=config.single_tab_mode,
                json_model_output=config.json_model_output,
                multiple_tools_per_call=config.multiple_tools_per_call,
                viewport_height=config.viewport_height,
                viewport_width=config.viewport_width,
                use_action_guard=config.use_action_guard,
            )
    
    @classmethod
    def from_config(cls, config: Union[WebSurferConfig, DeepSearchWebSurferConfig]) -> Self:
        """ä»é…ç½®åˆ›å»ºå®ä¾‹"""
        return cls._from_config(config)
    
    async def save_state(self) -> Dict[str, Any]:
        """ä¿å­˜çŠ¶æ€"""
        base_state = await super().save_state()
        
        deep_search_state = DeepSearchWebSurferState(
            **base_state,
            search_history=self.search_history,
            collected_information=self.collected_information,
            search_depth=self.current_search_depth,
            visited_urls=list(self.visited_urls),  # è½¬æ¢ä¸ºåˆ—è¡¨ä¿å­˜
            search_queue=self.search_queue, # ä¿å­˜é˜Ÿåˆ—
            searched_keywords=self.searched_keywords, # ä¿å­˜å·²æœç´¢å…³é”®è¯åˆ—è¡¨
            total_pages_visited=self.total_pages_visited, # ä¿å­˜æ€»è®¿é—®é¡µé¢æ•°
            page_results=self.page_results, # ä¿å­˜é¡µé¢æœç´¢ç»“æœ
        )
        
        return deep_search_state.model_dump()
    
    async def load_state(self, state: Mapping[str, Any]) -> None:
        """åŠ è½½çŠ¶æ€"""
        await super().load_state(state)
        
        deep_search_state = DeepSearchWebSurferState.model_validate(state)
        self.search_history = deep_search_state.search_history
        self.collected_information = deep_search_state.collected_information
        self.current_search_depth = deep_search_state.search_depth
        self.visited_urls = set(deep_search_state.visited_urls)  # è½¬æ¢ä¸ºé›†åˆ 
        self.search_queue = deep_search_state.search_queue # åŠ è½½é˜Ÿåˆ— 
        self.searched_keywords = deep_search_state.searched_keywords # åŠ è½½å·²æœç´¢å…³é”®è¯åˆ—è¡¨ 
        self.total_pages_visited = deep_search_state.total_pages_visited # åŠ è½½æ€»è®¿é—®é¡µé¢æ•° 
        self.page_results = deep_search_state.page_results # åŠ è½½é¡µé¢æœç´¢ç»“æœ 